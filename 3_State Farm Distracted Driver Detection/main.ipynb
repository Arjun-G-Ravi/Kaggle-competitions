{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/state-farm-distracted-driver-detection/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hello this is the RAM optimised branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# import cv2\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "# import torchvision\n",
    "import torch.nn.functional as F\n",
    "# import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriverDataset(Dataset):\n",
    "    def __init__(self, dir_path, train): # The last parameter is used to split up the data into train or inference\n",
    "        self.train = train\n",
    "        \n",
    "        self.x_ = []  \n",
    "        self.y_ = []  \n",
    "        if self.train:\n",
    "            for i in range(10):\n",
    "                sub_dir_path =f'{dir_path}/c{i}'\n",
    "                files = os.listdir(sub_dir_path)\n",
    "                for img_name in files:\n",
    "                    full_file_path = sub_dir_path + '/' + img_name\n",
    "                    self.x_.append(full_file_path)\n",
    "                    self.y_.append(i)\n",
    "        else:\n",
    "            files = os.listdir(dir_path)\n",
    "            for img_name in files:\n",
    "                    full_file_path = dir_path + '/' + img_name\n",
    "                    self.x_.append(full_file_path)\n",
    "                    # self.y_.append(i)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x_image = np.array(Image.open(self.x_[index]))\n",
    "        # print(x_image.shape)\n",
    "        x_image = x_image.reshape( (3, 480, 640) )\n",
    "        if self.train: return x_image, self.y_[index]\n",
    "        return x_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DriverDataset('data/imgs/train', True)\n",
    "train_share = 0.8\n",
    "train_dataset, val_dataset = random_split(dataset, [int(train_share*len(dataset)),len(dataset) -int(train_share*len(dataset)) ])\n",
    "test_dataset = DriverDataset('data/imgs/test', False)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.l1 = nn.Linear(16*158*118,640)\n",
    "        self.l2 = nn.Linear(640, 160)\n",
    "        self.l3 = nn.Linear(160, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x -> (n, 3, 640, 480)\n",
    "        out = self.pool(F.relu(self.conv1(x))) # -> (n, 6, 318, 238)\n",
    "        out = self.pool(F.relu(self.conv2(out))) # -> (n, 16, 158, 118)\n",
    "        out = out.view(-1, 16*158*118)\n",
    "        out = F.relu(self.l1(out))\n",
    "        out = F.relu(self.l2(out))\n",
    "        out = self.l3(out)\n",
    "        return out\n",
    "\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_category = nn.CrossEntropyLoss().to(device)\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732d61e897da46b396ca44c6250b6b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in trange(1):\n",
    "    for i, (x_, y_) in enumerate(train_loader):\n",
    "        x_ = x_.to(torch.float32).to(device)\n",
    "        y_ = F.one_hot(y_.clone().detach().long(), 10).float().to(device)\n",
    "        y_pred = model(x_)\n",
    "        loss = loss_category(y_pred, y_)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "    if (epoch+1)%5 == 0: print(f'Epoch {epoch+1}: {loss.item():3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc840eebe95462585aeb4b2a164657e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 10.32387535537098 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "score = 0\n",
    "tot = 0\n",
    "with torch.no_grad():\n",
    "    for i, (x_, y_) in enumerate(tqdm(train_loader, desc=\"Processing\", leave=False)):\n",
    "        x_ = x_.to(torch.float32).to(device)\n",
    "        y_ = y_.to(device)\n",
    "        y_pred = model(x_)\n",
    "        for y1, y_pred1 in zip(y_.int(), torch.argmax(y_pred, dim=1)):\n",
    "            if y1.item() == y_pred1.item():\n",
    "                score += 1\n",
    "            tot += 1\n",
    "print('Train acc:', score * 100 / tot, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2aeb6b7b684a3f9b2a9b97e0540576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 11.014492753623188 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "score = 0\n",
    "tot = 0\n",
    "with torch.no_grad():\n",
    "    for i, (x_, y_) in enumerate(tqdm(val_loader, desc=\"Processing\", leave=False)):\n",
    "        x_ = x_.to(torch.float32).to(device)\n",
    "        y_ = y_.to(device)\n",
    "        y_pred = model(x_)\n",
    "        for y1, y_pred1 in zip(y_.int(), torch.argmax(y_pred, dim=1)):\n",
    "            if y1.item() == y_pred1.item():\n",
    "                score += 1\n",
    "            tot += 1\n",
    "print('Train acc:', score * 100 / tot, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best acc: 75.6 % "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2306fde7ef42098644d60ba230719c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m         x_ \u001b[38;5;241m=\u001b[39m \u001b[43mx_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m model(x_)\n\u001b[1;32m      7\u001b[0m         softmax_output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(y_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "out = []\n",
    "with torch.no_grad():\n",
    "    for x_ in tqdm(test_loader, desc=\"Processing\"):\n",
    "        x_ = x_.to(torch.float32).to(device)\n",
    "        y_pred = model(x_)\n",
    "        softmax_output = F.softmax(y_pred, dim=1)\n",
    "        out.extend(list(softmax_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset.x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = []\n",
    "c1 = []\n",
    "c2 = []\n",
    "c3 = []\n",
    "c4 = []\n",
    "c5 = []\n",
    "c6 = []\n",
    "c7 = []\n",
    "c8 = []\n",
    "c9 = []\n",
    "\n",
    "for i in out:\n",
    "    c0.append(i[0].item())\n",
    "    c1.append(i[1].item())\n",
    "    c2.append(i[2].item())\n",
    "    c3.append(i[3].item())\n",
    "    c4.append(i[4].item())\n",
    "    c5.append(i[5].item())\n",
    "    c6.append(i[6].item())\n",
    "    c7.append(i[7].item())\n",
    "    c8.append(i[8].item())\n",
    "    c9.append(i[9].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {'img':[img.split('/')[-1] for img in test_dataset.x_], \n",
    "       'c0': c0,\n",
    "       'c1': c1,\n",
    "       'c2': c2,\n",
    "       'c3': c3,\n",
    "       'c4': c4,\n",
    "       'c5': c5,\n",
    "       'c6': c6,\n",
    "       'c7': c7,\n",
    "       'c8': c8,\n",
    "       'c9': c9,\n",
    "       }\n",
    "df = pd.DataFrame(out)\n",
    "df.to_csv('data/output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
