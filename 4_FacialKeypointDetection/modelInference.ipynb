{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self.X = (X/255.).view(-1, 1, 96,96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        df = pd.read_csv('data/IdLookupTable.csv')\n",
    "        X = []\n",
    "        face_feature = {\n",
    "    \"left_eye_center_x\": 1,\n",
    "    \"left_eye_center_y\": 2,\n",
    "    \"right_eye_center_x\": 3,\n",
    "    \"right_eye_center_y\": 4,\n",
    "    \"left_eye_inner_corner_x\": 5,\n",
    "    \"left_eye_inner_corner_y\": 6,\n",
    "    \"left_eye_outer_corner_x\": 7,\n",
    "    \"left_eye_outer_corner_y\": 8,\n",
    "    \"right_eye_inner_corner_x\": 9,\n",
    "    \"right_eye_inner_corner_y\": 10,\n",
    "    \"right_eye_outer_corner_x\": 11,\n",
    "    \"right_eye_outer_corner_y\": 12,\n",
    "    \"left_eyebrow_inner_end_x\": 13,\n",
    "    \"left_eyebrow_inner_end_y\": 14,\n",
    "    \"left_eyebrow_outer_end_x\": 15,\n",
    "    \"left_eyebrow_outer_end_y\": 16,\n",
    "    \"right_eyebrow_inner_end_x\": 17,\n",
    "    \"right_eyebrow_inner_end_y\": 18,\n",
    "    \"right_eyebrow_outer_end_x\": 19,\n",
    "    \"right_eyebrow_outer_end_y\": 20,\n",
    "    \"nose_tip_x\": 21,\n",
    "    \"nose_tip_y\": 22,\n",
    "    \"mouth_left_corner_x\": 23,\n",
    "    \"mouth_left_corner_y\": 24,\n",
    "    \"mouth_right_corner_x\": 25,\n",
    "    \"mouth_right_corner_y\": 26,\n",
    "    \"mouth_center_top_lip_x\": 27,\n",
    "    \"mouth_center_top_lip_y\": 28,\n",
    "    \"mouth_center_bottom_lip_x\": 29,\n",
    "    \"mouth_center_bottom_lip_y\": 30\n",
    "}\n",
    "\n",
    "        for i,j in zip(df['ImageId'], df['FeatureName']):\n",
    "            X.append((i,face_feature[j]))\n",
    "        self.X = X\n",
    "        # print(self.X)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index]\n",
    "\n",
    "test_dataset = FaceDataset()\n",
    "val_dataloader = DataLoader(dataset=test_dataset, batch_size=1,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 21 * 21, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 30)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 21 * 21)\n",
    "        x = torch.relu(self.fc1(x))  \n",
    "        x = torch.relu(self.fc2(x)) \n",
    "        x = self.fc3(x)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_122839/926347604.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('saves/model1.pt')\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('saves/model1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([1]) tensor([1])\n",
      "1\n",
      "76 87 81 72 65 59 64 76 69 42 31 38 49 58 58 47 37 33 32 33 35 50 55 54 50 51 61 78 92 100 101 79 55 47 52 50 47 39 38 52 46 25 25 39 52 63 59 67 79 68 48 44 44 43 44 45 53 56 55 50 48 54 58 51 52 67 72 86 76 64 75 97 97 83 85 81 67 53 45 39 33 33 53 73 60 44 47 72 98 121 120 117 127 120 115 117 68 85 84 72 63 52 57 69 69 49 34 40 46 57 63 58 45 39 30 31 35 47 52 47 52 53 54 70 84 93 104 94 67 50 48 49 48 39 35 48 53 44 33 35 47 66 73 73 73 69 58 48 42 41 51 52 54 58 51 49 53 60 59 49 46 61 72 87 82 66 71 93 108 95 83 85 81 64 58 54 42 33 48 66 57 45 53 72 88 110 125 125 134 132 111 106 63 78 84 76 64 52 53 58 56 47 37 41 48 57 64 66 56 44 38 28 32 41 49 42 46 56 56 64 75 83 91 97 82 60 49 50 47 42 38 43 56 60 52 41 48 62 73 72 63 61 64 56 44 44 45 50 50 53 59 63 64 62 60 51 51 60 74 87 85 60 65 86 104 105 88 86 86 74 64 65 56 49 58 68 66 55 57 74 88 98 116 127 139 133 110 106 65 73 80 75 65 59 54 53 48 41 36 39 50 58 69 72 61 49 46 41 37 39 42 42 42 51 58 61 69 75 75 87 91 69 52 53 50 42 40 44 50 56 62 64 68 71 70 69 59 50 58 65 61 57 50 50 58 69 74 68 69 66 63 59 53 54 73 92 84 61 60 82 102 97 92 87 83 78 67 65 67 66 70 73 68 62 63 79 92 95 108 126 142 138 122 115 70 71 64 61 63 60 54 50 47 39 36 39 53 60 66 69 59 50 47 53 48 48 46 45 46 48 55 61 65 65 67 72 83 74 58 56 57 54 49 43 43 49 51 55 56 63 65 65 62 52 53 63 70 64 63 62 69 70 69 66 70 72 70 66 50 51 75 92 80 67 66 83 102 87 78 87 89 77 71 71 70 57 55 66 72 67 66 88 95 95 106 124 147 147 110 107 66 69 54 44 49 56 58 52 45 43 40 38 44 55 62 63 56 48 47 49 51 52 56 54 50 50 54 59 64 62 57 60 70 76 63 56 56 58 62 59 53 51 52 51 55 57 58 60 61 53 53 59 62 60 61 67 66 66 67 73 76 73 67 59 56 65 80 83 76 67 65 81 99 83 68 82 92 86 82 81 69 61 63 65 67 70 80 98 96 91 105 121 142 147 107 96 59 67 52 36 30 37 53 57 47 45 47 50 51 53 54 53 51 50 51 45 45 43 48 51 54 54 53 57 59 60 55 54 61 74 71 59 57 58 62 69 72 68 68 67 67 61 57 62 62 52 48 54 54 55 53 60 64 71 77 80 76 66 66 63 63 75 84 79 73 67 65 76 94 85 67 73 85 91 85 82 74 63 68 66 62 77 100 100 85 91 103 112 131 145 136 109 50 56 48 38 29 22 34 49 47 46 59 64 57 50 44 46 48 47 52 52 38 42 45 49 47 46 50 54 55 53 57 56 62 70 76 68 68 68 67 67 71 76 75 79 76 75 68 64 59 49 42 45 46 44 55 66 74 74 75 72 70 72 68 64 68 78 82 82 71 66 69 78 84 79 70 68 75 81 81 82 81 73 72 70 81 98 99 88 86 102 95 101 127 137 136 122 49 53 48 40 35 28 31 41 45 48 73 79 63 53 40 40 45 43 43 52 43 35 45 54 49 45 49 54 60 58 62 65 66 69 73 71 69 71 76 75 70 73 73 80 84 77 71 68 65 59 52 48 46 42 55 73 88 90 82 77 74 68 56 59 70 77 82 82 71 67 73 78 81 77 68 64 64 70 77 77 78 77 80 84 92 96 99 104 106 101 89 100 127 137 131 128 46 55 47 38 38 36 30 40 48 45 75 93 75 57 40 39 40 39 41 49 53 40 40 47 58 60 64 70 70 78 81 84 83 83 86 88 75 73 80 84 83 79 82 80 79 80 86 83 80 78 64 63 61 58 54 62 76 92 91 81 70 65 70 72 75 79 84 76 69 67 73 78 78 72 65 65 64 73 76 74 74 80 82 89 98 104 116 111 92 85 88 96 117 134 130 128 46 57 49 38 45 43 29 45 61 45 73 102 80 60 47 41 40 38 43 53 56 49 46 48 55 65 69 81 89 95 104 105 105 102 98 97 96 87 86 87 89 89 87 82 84 90 88 85 88 86 79 78 79 75 69 67 74 83 90 88 80 77 76 78 80 83 81 73 66 66 70 76 76 69 67 65 62 71 79 78 77 74 81 93 102 108 103 86 72 76 84 95 111 127 130 121 46 53 48 37 42 46 36 41 60 52 76 104 77 56 50 50 48 41 47 57 58 53 51 52 51 62 76 87 100 104 111 113 118 117 115 111 111 109 102 95 92 95 90 93 96 101 98 94 96 99 101 99 98 89 87 85 85 82 87 89 82 76 78 82 81 89 79 69 61 63 73 82 76 73 73 62 62 76 81 72 69 69 75 84 91 90 80 72 68 76 80 89 108 122 123 123 46 49 48 36 44 55 43 40 57 56 72 101 76 53 49 51 52 48 51 56 63 57 54 54 57 66 82 92 98 105 109 115 120 127 129 128 121 120 117 113 109 106 110 111 112 113 116 105 107 114 115 114 109 105 98 100 99 97 89 87 84 80 82 82 83 84 76 66 63 67 81 83 77 83 76 58 60 77 78 77 75 74 75 81 83 85 80 65 69 81 81 80 97 113 115 119 40 44 51 38 40 56 49 44 56 58 70 90 72 53 49 50 59 62 59 58 64 59 55 56 60 79 92 98 100 107 107 111 115 123 127 128 130 126 125 124 123 118 120 123 126 123 122 116 113 119 118 118 116 111 105 107 108 106 94 83 79 76 75 75 79 74 72 65 66 71 81 86 84 75 62 71 73 71 74 81 78 75 75 77 80 79 69 61 65 66 62 61 64 86 107 109 35 43 51 39 42 58 53 41 55 64 69 75 67 53 47 54 66 66 59 65 71 69 54 55 65 84 95 98 102 99 103 108 118 121 128 128 130 131 131 131 127 127 126 130 126 130 127 121 115 121 128 127 125 117 111 101 97 97 95 85 80 79 78 79 72 68 63 65 66 72 81 82 75 75 85 89 86 78 82 85 85 81 81 82 79 74 65 59 56 53 54 61 57 58 84 103 34 35 40 38 48 57 50 42 51 61 66 69 62 53 51 61 70 69 66 71 72 60 46 57 76 88 98 103 104 99 104 112 123 126 126 130 132 136 134 134 131 132 131 130 127 132 138 132 123 121 127 128 126 115 107 98 95 93 91 86 86 82 79 80 75 71 67 66 74 82 83 80 87 93 95 92 90 90 93 93 90 87 88 87 78 78 75 61 52 53 49 50 59 59 64 76 34 26 31 36 46 55 47 38 46 56 59 67 62 53 53 62 76 78 78 63 49 44 57 72 88 95 103 112 107 105 103 113 121 129 130 132 127 129 132 126 131 138 139 129 123 123 130 128 122 117 116 118 113 113 112 115 111 107 103 102 94 85 81 72 71 73 72 70 80 86 88 86 92 99 97 99 94 96 94 95 92 90 90 88 85 82 78 70 65 60 47 37 47 57 61 63 33 24 25 29 39 49 44 34 45 55 59 61 57 49 55 69 81 86 83 72 61 65 76 86 91 94 97 101 102 103 104 106 112 118 123 122 122 117 119 119 120 127 130 127 117 115 119 123 118 116 117 117 123 128 130 128 127 123 119 112 104 95 86 76 74 76 78 84 88 89 90 94 96 99 97 99 100 98 97 95 94 96 103 99 92 88 82 76 69 60 50 41 43 50 45 46 29 22 21 23 31 42 46 39 46 57 60 56 55 51 60 81 86 91 91 93 87 87 85 88 92 94 95 95 100 103 112 112 112 111 116 116 119 123 123 124 122 127 129 133 131 131 132 135 127 124 134 131 133 132 135 133 132 125 116 109 105 102 92 88 86 88 87 91 90 94 94 98 104 104 105 104 108 105 108 107 108 110 115 113 101 95 91 78 63 61 54 47 50 51 44 43 26 20 26 29 33 40 40 40 49 60 61 61 56 55 71 89 91 92 97 95 89 85 87 86 92 95 96 97 96 102 112 119 115 123 129 129 129 135 135 135 136 137 141 139 138 134 137 134 131 127 129 129 128 128 132 135 128 119 108 107 105 103 97 96 100 99 97 91 94 93 94 95 103 109 110 112 111 113 113 115 114 114 118 119 110 102 90 75 74 76 67 52 47 48 47 51 28 25 27 29 34 45 44 43 48 58 58 55 54 64 81 89 95 94 98 97 92 85 88 92 95 99 98 100 100 107 111 117 120 131 134 130 131 136 137 135 138 138 138 135 136 135 133 135 137 135 125 122 119 122 121 122 119 114 111 112 115 113 113 113 114 107 105 102 104 99 95 97 102 109 112 115 110 113 117 122 122 121 123 117 116 113 99 82 77 80 72 55 53 53 46 48 37 32 30 30 35 44 51 50 55 60 60 54 57 72 86 90 91 98 98 98 94 95 97 106 109 106 103 104 110 109 113 113 122 127 127 123 128 132 131 130 134 138 135 138 136 134 132 133 136 138 132 120 116 117 117 118 125 123 122 122 124 127 129 127 120 117 110 110 108 107 99 103 108 110 112 111 116 117 122 127 131 130 131 131 132 130 113 97 95 94 75 59 57 53 44 42 36 34 33 31 35 42 49 52 58 61 61 60 66 77 84 91 91 98 97 98 100 100 105 109 111 111 114 112 116 120 120 116 115 121 122 119 117 119 124 133 137 140 137 136 133 129 130 130 136 132 123 119 122 118 118 125 129 127 122 122 124 129 126 126 123 120 117 114 113 113 110 111 114 113 112 109 112 117 124 130 138 142 141 139 136 138 122 105 102 102 88 69 56 48 41 37 39 33 32 33 35 42 46 57 59 60 60 64 70 78 85 88 93 96 100 98 102 100 105 107 106 109 112 116 117 122 121 121 118 125 125 118 110 111 124 130 132 134 134 128 127 126 127 126 128 126 122 122 116 113 111 121 124 123 122 125 128 127 126 120 118 118 120 112 112 110 112 113 112 114 112 105 105 118 125 132 142 143 140 141 142 143 135 118 104 96 85 66 53 48 45 42 43 39 37 35 35 43 51 59 59 59 68 75 80 81 87 88 93 97 99 99 98 102 105 107 106 110 112 114 116 119 122 122 122 121 125 122 114 106 116 125 126 128 127 125 124 127 126 122 116 122 123 121 117 111 108 111 119 120 117 117 123 123 122 118 115 114 113 111 105 104 104 105 106 106 107 110 109 111 124 135 146 147 144 140 139 139 138 134 116 98 85 69 53 43 44 47 37 34 29 25 32 42 49 55 65 73 79 81 84 88 88 91 91 94 92 91 91 97 102 104 109 105 105 111 116 116 119 117 119 117 118 119 115 110 108 113 114 120 122 123 118 115 110 108 110 111 113 110 110 107 105 110 121 119 112 110 111 110 105 105 100 101 100 97 91 90 91 90 91 93 97 103 113 110 119 136 145 150 149 144 140 140 139 140 128 106 88 72 58 42 38 41 32 27 19 18 29 44 50 61 73 78 79 86 87 89 90 90 91 87 81 75 77 81 84 84 89 92 94 99 100 106 110 112 109 111 112 117 115 111 105 99 99 106 118 118 113 106 107 107 104 100 99 102 104 109 107 114 118 114 102 100 98 91 87 83 80 79 79 73 72 69 69 69 74 80 83 88 98 106 120 138 145 147 148 144 138 139 139 139 130 107 86 70 58 39 31 43 30 22 19 24 32 41 48 65 80 83 83 86 92 93 90 84 78 66 57 55 64 66 64 68 69 71 73 78 82 90 90 94 98 106 108 109 112 108 105 95 98 109 114 111 105 97 94 97 102 105 100 100 94 96 100 109 112 106 94 86 83 76 71 67 63 60 60 52 50 51 52 57 61 71 76 73 72 82 102 126 139 147 152 147 141 139 141 142 137 112 84 67 55 33 20 46 27 23 24 28 31 34 48 69 81 83 85 88 96 97 88 71 56 50 49 54 58 58 53 56 52 47 47 48 54 60 69 72 81 88 97 100 101 102 94 88 92 105 109 105 96 93 87 87 94 93 90 88 89 83 86 93 96 87 80 71 62 59 54 49 46 48 49 50 49 50 53 64 71 75 77 74 76 76 91 115 134 142 145 143 139 136 134 139 138 113 82 64 53 32 13 49 28 26 24 21 24 33 50 70 82 85 87 93 100 92 73 62 59 63 68 68 66 63 57 50 50 45 41 36 37 39 46 47 52 66 78 86 86 87 84 85 87 98 102 96 89 89 91 91 92 90 88 84 84 80 79 80 78 73 70 66 58 51 43 43 43 46 49 51 57 63 72 83 94 99 99 99 103 101 101 108 119 131 138 139 138 137 137 142 137 112 81 61 49 36 34 65 31 32 25 18 18 28 47 66 79 88 89 95 96 81 66 69 78 88 98 99 102 98 94 90 85 76 65 53 42 38 36 36 38 47 56 66 73 75 78 79 85 89 95 91 91 91 94 95 96 95 86 83 79 83 80 75 69 68 66 61 60 60 60 59 59 59 66 72 83 90 98 99 93 92 90 96 101 109 109 107 110 120 132 139 142 141 137 139 137 111 80 63 49 39 48 64 37 38 42 32 17 23 44 64 79 88 89 90 89 82 80 82 88 90 90 92 93 95 94 96 94 87 76 70 64 58 57 57 55 52 52 58 64 63 70 73 79 82 82 90 96 100 101 104 102 98 88 80 76 75 73 68 64 62 65 67 73 73 71 64 60 62 62 65 67 69 76 82 84 85 84 87 94 97 95 93 100 108 123 137 143 143 140 141 138 111 82 69 57 48 50 60 37 31 36 24 10 21 37 59 83 89 89 89 85 77 72 75 82 82 79 84 84 85 84 81 77 70 64 60 60 58 58 61 64 62 62 62 60 62 62 70 74 84 84 91 99 108 105 108 111 101 91 82 81 72 66 63 68 73 82 84 74 64 65 64 58 61 64 59 63 64 58 52 56 66 81 86 92 92 92 93 100 110 118 131 140 142 143 143 138 117 89 72 60 58 55 51 41 34 31 16 6 14 30 57 82 92 91 91 84 75 71 72 75 74 78 81 77 69 61 55 56 66 68 57 50 47 52 58 60 59 58 64 62 66 70 70 74 78 85 89 100 108 111 106 105 102 96 91 81 73 68 70 74 82 80 72 61 65 71 65 48 35 32 31 33 34 36 36 46 52 54 60 71 84 90 94 98 102 112 122 134 138 138 139 139 120 91 75 66 64 65 51 48 41 32 18 10 17 32 58 82 93 94 91 84 80 79 77 78 76 77 70 52 42 38 37 37 35 29 25 24 29 37 52 65 66 62 61 60 56 65 75 79 77 83 90 103 114 116 110 102 103 101 98 85 77 74 72 68 71 65 57 70 75 68 55 36 29 31 29 23 17 23 33 30 40 52 46 46 65 81 89 92 99 107 114 127 138 136 136 137 125 98 81 78 77 65 50 56 46 34 20 18 24 36 59 84 94 91 87 87 81 78 77 76 71 56 39 30 23 22 31 24 16 23 27 27 26 37 57 68 69 75 75 73 63 58 70 77 82 82 93 101 109 109 109 103 97 103 102 90 79 77 70 72 77 81 84 76 71 100 94 37 45 83 64 38 22 44 86 72 32 29 38 42 49 64 78 88 94 102 113 123 129 129 132 137 127 104 87 76 74 58 45 63 54 41 28 23 30 44 64 83 95 91 87 87 86 81 81 78 62 38 25 14 17 66 94 47 19 62 69 41 27 38 82 96 75 74 86 94 94 75 66 73 80 83 89 96 99 98 94 96 98 104 101 88 80 76 67 83 108 105 86 68 89 138 112 43 38 46 33 28 17 46 106 121 80 42 33 40 49 61 80 87 95 101 111 119 122 124 131 137 128 108 83 69 70 54 41 65 65 53 39 24 35 50 68 88 97 92 87 84 84 84 78 69 48 28 17 21 60 109 100 42 21 51 53 38 31 35 90 120 93 74 84 103 108 91 72 73 77 80 82 91 98 103 97 95 98 96 93 83 83 72 70 94 113 94 77 80 98 130 135 80 33 27 26 18 24 75 122 119 87 74 79 71 73 79 86 91 96 100 102 109 117 126 133 140 131 107 78 67 72 64 50 61 71 60 50 35 41 55 68 90 97 91 89 87 84 84 82 76 60 41 41 52 75 98 105 70 28 12 15 23 30 63 118 124 95 90 89 85 100 111 89 75 78 83 88 89 94 95 93 92 98 102 99 86 79 72 76 104 103 84 78 77 75 89 118 119 76 45 34 38 75 109 110 89 79 103 126 115 99 91 91 96 98 101 102 107 123 134 139 141 134 107 78 68 77 67 48 61 73 65 52 40 37 51 69 92 95 90 89 90 89 86 86 80 71 60 60 64 73 93 114 116 82 45 31 28 61 108 114 92 76 83 95 98 104 108 94 78 80 79 88 93 97 95 100 110 108 104 98 90 79 70 75 98 108 113 110 93 84 85 93 100 96 82 72 81 93 90 78 76 94 118 124 121 116 108 99 103 108 113 114 117 129 138 142 141 130 99 74 70 83 71 47 60 71 70 54 34 34 51 74 91 96 90 85 87 86 85 84 79 76 79 76 73 74 83 93 105 108 100 88 82 88 91 88 85 88 94 109 114 105 98 86 79 78 80 87 97 103 108 109 115 114 105 99 91 83 75 78 85 101 114 117 109 100 99 99 91 83 81 73 72 69 70 76 86 103 116 123 127 125 115 111 108 110 112 109 111 125 138 139 138 125 92 74 83 89 67 44 69 77 71 57 35 32 49 70 93 98 89 86 92 93 93 91 90 90 94 89 82 74 67 68 76 87 87 85 85 86 87 93 93 95 103 104 105 101 94 87 84 76 77 87 95 104 114 115 109 104 100 101 91 82 82 82 85 92 100 102 103 105 109 107 102 91 85 83 83 81 84 92 99 111 117 125 130 124 114 113 117 117 116 109 110 120 130 131 139 124 89 84 93 92 61 42 68 80 74 60 37 26 45 64 87 94 93 88 90 92 94 98 99 99 96 94 89 84 78 80 81 82 79 77 84 93 98 95 96 100 102 103 106 100 92 90 85 74 73 90 98 104 108 110 105 96 100 101 94 80 80 83 88 89 94 96 93 93 101 109 114 113 106 107 105 105 104 105 110 116 120 123 124 119 115 113 118 116 114 111 116 129 133 134 133 115 84 82 91 88 53 48 82 75 75 67 42 21 46 69 86 94 90 89 88 93 100 107 102 97 96 98 96 94 92 90 95 93 95 96 100 105 108 103 102 106 105 108 105 101 95 94 84 69 72 86 98 101 102 105 107 103 103 102 96 82 75 83 88 93 93 99 101 95 92 100 103 108 114 115 116 111 110 109 109 112 117 121 118 112 111 113 115 115 109 110 121 131 131 131 128 111 87 79 89 78 49 57 103 81 79 74 45 19 45 84 90 92 89 88 87 94 102 103 102 100 102 100 100 98 97 98 101 103 103 106 107 111 110 110 106 106 109 108 102 97 99 96 85 68 75 85 91 88 91 95 95 98 91 97 105 97 76 80 87 95 98 97 100 105 104 98 97 98 104 107 108 105 106 107 106 111 112 116 113 112 110 117 120 118 112 111 123 131 134 132 130 110 83 71 90 74 47 65 114 98 86 75 45 14 38 83 96 91 90 85 86 93 102 104 103 106 103 99 96 98 98 100 99 99 103 109 109 109 107 108 111 109 110 102 101 99 98 94 85 74 73 83 86 88 88 92 93 96 95 99 110 105 86 78 88 96 102 102 101 105 110 114 111 105 103 102 100 102 103 103 102 105 105 106 108 107 112 113 116 115 116 119 127 135 137 137 129 97 70 79 89 62 47 68 117 118 92 78 47 11 31 73 93 93 91 89 89 94 97 103 107 109 106 104 97 100 103 101 101 100 102 102 103 104 109 111 113 115 109 103 103 105 99 91 82 80 78 87 93 94 94 98 102 98 99 97 104 104 95 85 89 95 101 105 105 105 105 115 116 115 111 108 105 103 102 103 107 109 112 110 115 113 116 116 114 110 114 122 128 136 135 140 124 78 65 96 82 45 52 72 95 119 99 82 48 10 30 70 86 89 90 94 91 93 96 98 104 107 112 110 107 101 99 97 98 99 98 100 105 110 111 112 111 108 104 103 104 104 97 87 85 83 83 87 94 95 102 102 105 106 101 99 107 116 103 92 88 91 93 102 109 107 106 108 114 117 116 114 116 112 112 112 113 115 115 117 115 117 117 116 110 110 114 119 126 129 132 142 116 68 85 106 63 40 61 79 83 108 104 86 52 15 27 63 82 88 89 92 93 92 93 93 104 108 111 111 111 108 105 104 100 100 99 103 108 112 115 111 105 105 106 102 104 98 90 85 84 80 83 87 90 94 98 102 105 110 101 97 109 118 108 97 91 87 87 97 106 107 112 110 111 110 111 113 118 114 113 117 116 118 116 118 116 115 110 106 110 114 118 121 120 120 133 139 112 89 105 101 50 43 65 82 91 93 99 88 48 15 25 47 74 88 91 92 90 89 88 94 103 112 115 119 114 114 111 107 104 103 104 107 113 115 116 109 107 104 107 104 99 92 87 88 84 84 85 89 89 95 102 107 103 97 96 92 98 103 106 101 97 91 93 96 99 109 112 115 113 112 107 113 113 113 115 119 119 118 119 116 117 116 115 107 111 115 113 115 117 121 131 136 113 102 112 88 45 53 76 90 106 93 98 84 46 18 25 38 61 89 93 90 91 92 89 91 97 107 114 115 116 115 114 116 119 113 116 122 120 114 109 106 107 109 108 104 95 89 90 90 86 83 84 89 90 91 98 95 95 93 92 89 91 100 106 106 103 102 100 101 103 109 110 114 115 115 115 111 109 108 113 117 120 119 118 116 113 108 106 107 108 111 111 112 112 118 136 130 106 137 144 80 37 60 87 90 110 91 94 85 42 15 28 38 66 94 92 89 90 89 93 91 94 99 111 111 117 119 120 119 115 119 121 124 114 109 108 111 109 111 109 101 96 91 91 86 85 81 85 90 96 93 90 93 96 100 92 91 92 103 108 112 109 101 98 100 107 108 113 113 117 115 116 112 113 111 113 117 117 118 116 120 110 105 102 107 108 108 109 110 113 119 135 105 101 163 139 66 44 73 89 90 115 90 88 76 36 21 36 42 62 92 98 92 90 86 90 90 90 95 99 104 109 115 115 116 113 118 120 116 110 110 112 111 114 112 109 100 98 94 90 86 82 79 84 92 91 94 88 94 99 101 96 95 100 106 113 114 108 97 93 91 97 108 115 117 114 117 119 118 116 118 116 118 116 116 117 119 118 111 109 104 106 106 106 106 109 119 129 103 121 182 181 95 48 77 84 111 116 94 87 73 38 19 36 34 53 115 117 88 86 87 86 88 87 92 92 99 104 108 114 116 116 113 115 110 112 111 113 112 112 112 111 108 99 92 86 85 79 85 89 94 95 94 93 95 101 100 103 100 101 102 113 124 111 91 83 82 88 108 112 118 118 120 120 120 117 118 116 114 117 115 118 117 118 111 107 104 105 107 101 105 107 119 124 105 144 185 136 60 54 69 98 144 115 95 82 60 29 20 36 21 74 186 151 85 83 87 85 84 89 90 94 93 103 113 115 116 119 115 111 113 110 113 113 115 111 114 115 114 105 92 82 77 78 82 92 94 96 95 100 100 102 103 106 105 101 105 116 133 117 93 78 69 77 96 110 116 122 121 119 116 117 117 115 114 116 118 118 118 113 112 108 110 102 101 102 103 107 120 124 125 147 121 66 49 55 74 120 140 115 91 75 51 23 23 40 20 15 125 180 108 78 84 86 84 88 87 90 95 104 113 113 116 115 117 112 115 114 117 117 116 117 116 117 111 101 88 82 75 76 84 92 94 96 97 94 96 104 109 107 106 104 109 123 142 133 101 82 68 68 83 101 108 113 114 116 118 118 116 117 118 112 114 112 114 110 111 111 107 104 99 101 101 106 117 127 169 184 95 40 45 71 103 116 108 99 82 65 46 27 24 32 22 4 39 115 107 82 84 81 83 83 90 90 95 102 114 115 116 115 115 115 112 115 111 115 113 117 112 104 100 92 83 79 80 81 89 90 95 95 99 101 103 105 104 109 108 109 111 124 145 142 115 95 75 64 74 89 103 105 113 113 116 116 114 113 112 112 111 117 115 111 106 108 106 106 104 103 104 106 130 133 89 56 54 57 76 99 98 96 102 82 73 60 44 38 34 34 31 21 11 24 64 83 85 85 87 85 89 94 94 102 109 110 109 111 111 111 110 110 110 111 114 112 108 100 99 88 79 78 82 85 89 91 94 100 105 111 113 116 115 118 117 113 115 126 139 137 128 115 85 66 77 84 94 103 110 116 114 117 117 117 111 111 112 115 108 104 105 104 105 104 102 102 101 112 147 128 43 17 48 72 87 89 89 119 140 79 73 68 64 57 52 46 40 33 22 11 40 79 85 83 84 89 86 92 93 97 103 110 111 106 109 110 113 110 113 113 113 106 102 96 95 91 83 82 81 85 90 94 94 101 109 118 131 135 133 131 129 117 115 125 132 125 122 120 96 76 81 92 92 102 109 115 112 114 117 114 112 109 109 102 101 96 102 103 103 105 101 101 100 118 156 126 43 27 45 70 90 95 104 142 154 82 76 68 59 50 48 39 30 29 21 9 28 72 83 81 85 86 90 95 99 98 100 106 110 106 109 112 114 114 110 111 109 106 96 96 101 104 91 83 82 81 85 90 92 97 109 121 126 124 121 120 117 110 107 115 117 111 105 100 87 83 88 94 97 99 107 112 115 111 111 109 110 108 104 102 99 101 101 104 103 103 101 102 99 116 162 124 35 21 46 68 96 113 113 149 171 78 65 52 38 35 38 33 28 26 19 6 21 64 85 85 87 85 89 94 101 98 96 100 105 110 110 113 112 113 108 111 108 101 98 100 107 107 99 89 87 84 86 87 90 96 102 105 103 104 101 100 101 105 105 105 104 101 96 89 86 90 96 95 100 99 103 109 112 111 109 107 105 108 105 109 102 98 97 101 105 104 103 103 99 126 167 112 24 12 38 65 94 116 139 174 172 71 52 41 36 37 36 33 35 33 23 8 12 54 82 83 84 86 86 94 97 95 90 96 101 105 109 110 110 108 110 109 108 101 103 107 108 106 106 98 87 82 76 75 79 88 94 99 99 98 94 93 99 100 100 95 91 80 81 88 96 99 98 98 98 102 102 106 105 109 109 108 109 108 111 111 105 97 96 96 103 100 99 104 103 130 165 100 10 3 20 52 93 125 138 137 129 62 46 35 32 32 37 42 42 30 17 11 9 41 78 85 84 84 86 90 91 90 89 94 103 102 103 108 112 108 109 109 106 104 107 108 105 105 100 100 97 89 77 67 69 77 87 93 95 92 91 87 93 94 90 80 74 80 87 97 101 101 98 103 104 101 101 107 110 111 109 108 111 108 110 108 103 100 101 103 103 101 102 108 110 137 153 84 10 3 10 25 52 72 66 82 129 52 40 34 32 39 45 43 39 28 19 16 8 29 72 85 84 83 85 83 86 87 91 94 99 101 101 106 99 99 103 111 108 108 107 107 104 102 99 96 97 96 95 80 73 74 80 85 84 82 83 90 92 86 76 77 81 89 93 99 102 102 102 102 105 100 103 104 107 106 104 105 106 107 106 107 100 101 101 103 101 101 105 106 111 143 146 69 33 27 11 8 18 29 46 90 133 49 37 31 38 48 47 41 37 26 22 21 10 16 63 85 84 85 85 86 86 87 88 91 98 100 101 106 94 93 104 109 110 107 107 105 105 100 101 97 95 96 95 87 81 84 86 86 83 79 83 90 87 83 85 89 93 91 94 95 100 103 105 104 104 102 101 103 104 104 104 106 105 107 105 103 101 102 104 103 103 102 106 103 112 147 129 52 44 61 45 26 26 45 73 94 101 39 31 37 48 46 42 40 36 27 25 20 9 13 56 87 85 85 85 87 83 85 85 91 96 95 98 103 107 104 108 106 109 108 107 107 108 105 96 94 90 96 94 90 87 91 96 99 97 87 87 93 98 96 97 94 99 96 96 100 102 106 106 110 104 103 100 104 102 104 105 105 105 102 102 98 100 101 105 103 104 104 105 106 116 146 106 31 39 65 86 68 26 25 52 105 137 32 38 45 46 44 42 41 36 32 28 20 10 12 44 79 84 83 86 84 86 82 84 91 96 95 97 101 103 104 103 108 109 110 106 108 109 109 100 96 96 94 92 91 93 93 100 104 102 92 91 101 102 100 98 97 98 96 95 99 99 100 102 106 103 97 102 103 104 101 104 101 96 92 94 96 94 100 105 107 106 105 104 103 123 144 79 19 46 65 95 113 70 36 36 59 92 41 45 50 48 46 45 43 35 30 24 18 13 10 35 70 85 83 84 83 84 84 87 91 92 93 93 98 97 102 102 106 105 106 104 104 105 105 105 97 97 93 92 91 96 100 102 103 99 95 95 103 100 104 105 102 96 96 98 98 99 99 104 102 102 98 100 100 102 101 101 103 95 90 92 97 92 99 105 105 105 107 105 107 130 136 64 14 47 68 88 130 138 90 50 48 53 50 53 50 45 45 49 43 33 27 23 20 20 18 32 69 85 83 86 89 82 81 83 88 86 87 92 94 99 101 105 104 105 100 104 106 105 102 96 93 90 89 89 95 99 103 100 100 100 94 97 101 103 98 100 98 97 94 97 100 101 101 102 103 98 99 98 101 98 99 96 97 96 91 91 92 95 97 104 105 104 103 103 114 137 122 45 18 51 76 89 124 158 163 123 71 51 48 48 46 46 48 49 38 31 30 26 24 23 27 34 62 87 87 84 86 85 81 82 85 88 87 87 88 95 99 104 103 102 102 99 100 101 102 96 94 93 93 88 90 91 95 94 96 101 93 96 101 99 92 94 92 88 88 89 96 97 97 97 102 95 94 96 96 93 91 93 91 93 91 90 89 93 97 102 106 108 107 105 118 143 105 30 23 57 82 92 123 154 172 174 147 103 57 55 55 54 52 41 37 40 39 31 25 23 30 29 40 75 88 86 84 85 81 84 85 85 81 79 84 87 96 100 103 98 96 94 94 96 95 91 94 98 90 84 83 87 84 88 93 95 91 90 97 97 97 91 89 84 85 90 93 89 89 92 91 93 88 92 91 94 92 89 88 89 91 86 89 88 92 99 104 105 105 106 123 139 88 22 22 61 88 95 120 163 182 163 156 171 63 62 57 53 49 41 46 47 35 31 29 31 38 28 24 59 83 84 82 82 85 83 83 82 81 79 80 83 90 97 101 96 93 92 90 92 89 89 91 93 90 89 93 96 98 97 99 92 90 92 99 106 103 96 93 96 91 96 99 91 85 90 92 91 85 88 92 92 94 86 86 87 87 87 89 88 92 98 102 105 107 108 127 129 66 22 30 59 95 108 125 154 176 178 168 183 72 65 57 55 50 52 47 36 32 32 29 31 38 28 23 48 83 88 86 82 81 81 80 79 78 81 79 85 88 98 98 99 92 91 90 88 86 83 90 92 95 92 96 95 96 100 97 94 94 102 107 108 93 87 85 86 83 84 92 96 96 91 91 87 87 85 87 84 87 85 81 84 86 90 87 90 89 96 104 107 107 111 133 113 43 24 38 63 104 120 125 151 164 169 177 174 86 72 60 61 58 48 41 37 32 31 34 42 43 32 18 30 72 88 85 86 82 81 81 82 79 79 81 81 88 94 94 92 89 87 87 85 83 83 85 84 79 78 78 79 76 77 72 80 93 101 97 83 75 70 71 69 69 67 73 76 78 82 84 88 83 82 83 84 82 82 79 82 87 90 87 87 93 96 102 108 110 120 136 88 21 20 37 62 106 147 140 145 183 183 178 176 92 75 71 77 64 48 41 39 36 32 40 52 46 36 18 13 52 87 85 84 81 80 81 80 80 78 82 77 83 87 87 85 87 87 84 86 80 75 69 73 69 69 69 70 69 65 62 61 71 68 67 65 68 66 66 69 63 60 60 61 58 62 61 65 67 76 83 85 87 85 80 81 92 94 93 89 92 100 108 110 113 131 122 55 16 26 43 65 97 146 168 152 173 209 206 182 91 88 84 67 49 42 38 38 36 38 48 52 44 34 18 9 38 76 86 82 84 80 77 79 79 77 79 80 78 83 84 86 84 81 74 65 59 55 59 60 61 65 80 83 86 85 79 69 65 66 68 77 85 90 90 88 61 44 58 67 58 46 35 31 50 62 76 81 84 78 75 82 91 99 100 97 92 100 106 108 117 140 99 23 18 32 43 59 85 127 176 165 113 144 189 183 95 100 81 51 43 42 40 39 36 43 50 46 41 29 22 18 19 55 83 85 85 80 77 80 79 77 78 83 83 86 85 84 76 63 46 27 19 16 43 71 68 63 73 80 86 99 105 103 88 85 93 98 88 78 66 59 71 78 82 89 70 34 29 47 72 81 79 78 77 79 80 91 96 100 100 99 97 101 105 105 125 129 58 9 23 29 33 48 82 123 151 161 134 108 122 132 71 73 67 58 55 47 41 37 43 51 51 49 37 27 28 19 6 33 74 89 84 80 78 80 79 79 86 84 87 83 80 76 73 71 66 46 23 20 54 88 88 88 77 54 48 48 43 41 42 40 36 37 52 65 58 65 112 120 90 73 53 37 48 73 86 88 79 75 75 80 86 92 99 100 102 97 99 101 104 112 126 96 29 13 23 30 33 39 78 137 145 140 145 134 116 102 85 89 91 76 58 46 39 48 57 55 51 44 30 27 26 18 7 11 54 86 86 84 81 81 83 84 88 89 88 84 78 76 77 90 90 72 51 41 41 49 75 108 111 101 97 82 62 47 40 44 54 65 95 122 88 72 117 116 67 47 48 56 67 82 86 81 80 73 71 75 83 89 93 97 101 102 104 104 105 120 119 58 19 18 20 32 42 39 64 128 150 138 134 134 124 119 106 101 94 75 56 44 48 59 57 52 46 32 27 25 21 20 16 2 26 77 90 85 82 80 84 86 84 87 84 86 82 76 77 84 85 77 67 58 51 39 58 88 89 89 110 105 96 107 87 86 111 95 96 125 87 54 74 69 54 57 64 63 77 82 83 78 77 74 76 79 83 89 90 99 104 106 105 103 113 126 88 29 17 18 23 35 43 44 59 108 154 149 123 115 112 101 102 101 87 65 50 53 64 60 49 46 35 25 28 30 28 23 12 2 11 54 86 89 82 82 84 87 89 87 86 84 82 80 82 78 80 78 69 65 65 61 53 58 55 59 75 76 71 87 76 75 96 80 70 78 67 48 48 53 61 68 69 68 77 84 82 82 78 76 75 79 86 86 92 101 105 101 103 105 127 115 41 14 24 21 23 32 40 46 54 82 118 141 136 107 95 86 102 90 65 57 66 70 60 46 43 33 24 28 32 29 22 13 5 4 7 34 73 86 84 81 87 90 90 85 88 83 80 78 78 78 76 77 70 66 65 73 69 65 63 59 58 57 57 54 53 54 59 56 56 55 56 61 67 74 71 70 70 78 82 85 78 81 82 78 75 79 84 84 93 103 104 99 100 111 126 87 19 8 23 25 24 33 39 44 53 71 91 106 114 108 93 79 78 65 72 82 70 49 37 40 37 29 31 34 24 16 14 9 7 10 5 11 51 85 89 86 83 84 84 85 80 81 80 80 78 79 75 71 71 68 67 68 73 73 75 73 77 72 67 65 64 64 65 70 70 79 76 79 79 83 77 69 73 81 88 81 80 77 76 71 72 74 81 90 94 101 99 98 100 116 115 73 31 18 25 28 25 34 41 37 45 59 74 91 94 93 84 69 78 86 84 76 61 51 46 43 38 34 34 28 20 16 14 13 13 15 11 5 31 74 89 87 84 82 81 84 82 80 77 76 78 80 80 74 71 67 70 66 73 82 80 87 88 88 82 82 78 81 81 86 90 91 89 84 81 73 70 70 80 84 85 83 83 82 80 77 75 76 78 86 93 101 100 98 107 117 94 79 79 53 28 23 25 34 41 38 39 49 57 65 76 73 68 74 84 86 82 76 67 57 60 61 56 50 42 30 23 19 18 18 17 18 16 17 24 53 82 86 84 82 84 82 84 79 78 76 79 80 78 76 67 67 69 74 74 80 80 86 90 93 89 86 85 83 90 92 95 89 88 79 72 70 72 82 86 85 78 79 79 80 77 80 78 80 86 84 92 98 99 99 114 110 69 51 74 74 45 28 26 28 35 37 34 45 51 52 59 59 66 73 83 80 76 66 59 69 83 83 76 65 50 39 32 31 34 42 38 24 18 18 13 31 69 86 85 83 83 82 81 81 77 75 74 75 76 75 69 68 70 76 76 77 82 81 86 83 85 83 86 82 82 85 83 82 77 74 70 78 80 83 83 81 77 77 79 76 78 79 78 81 86 87 93 97 97 104 114 102 63 39 54 65 57 45 36 33 35 38 33 38 54 61 61 60 60 56 77 63 62 67 80 88 91 88 70 66 64 48 39 39 49 50 38 25 21 22 25 35 62 84 84 80 81 84 80 81 77 78 73 74 77 75 75 69 69 66 76 79 84 85 86 84 81 83 79 79 79 81 77 82 79 81 80 76 77 78 82 77 78 73 76 78 81 79 77 81 85 90 95 98 98 108 103 91 77 61 55 64 69 55 46 45 40 36 34 27 40 58 56 52 53 51 91 56 43 63 74 68 67 64 54 54 53 44 39 38 32 28 30 31 31 35 35 42 58 76 84 82 81 81 85 83 79 79 80 77 76 74 75 74 71 74 71 72 76 80 76 81 79 79 81 84 84 77 76 76 79 76 77 75 77 77 79 79 77 76 77 84 79 79 80 83 89 93 96 97 104 104 95 86 81 79 75 78 85 78 69 77 86 72 54 41 29 36 55 59 54 49 111 78 51 41 36 44 55 55 52 50 51 50 43 33 26 27 30 35 35 38 40 49 61 70 83 85 83 80 85 82 81 78 79 77 72 71 69 77 77 80 75 77 73 73 72 72 74 73 80 76 76 74 78 76 76 77 79 79 76 74 72 78 75 76 78 83 79 77 82 86 94 97 99 100 104 97 90 87 85 88 79 72 89 95 93 101 113 105 73 72 68 48 55 67 65 64 93 99 89 68 57 55 58 62 59 60 59 52 40 31 29 31 35 37 42 43 47 53 64 73 79 84 82 83 84 83 82 81 74 74 73 72 71 69 74 76 80 79 81 80 82 79 77 77 78 77 75 78 78 79 75 78 78 80 75 68 73 71 73 74 80 79 80 82 85 87 94 99 99 102 93 89 86 87 87 96 95 68 57 78 99 120 138 137 104 81 103 108 90 87 83 96 112 108 88 67 58 66 68 63 62 60 55 49 36 24 30 32 35 37 41 47 49 57 67 76 78 81 82 81 76 80 82 80 77 73 74 72 74 70 72 75 78 81 86 87 86 83 79 79 78 78 76 78 76 78 78 74 75 75 75 72 74 70 73 77 81 84 84 84 85 91 93 99 100 94 85 87 89 89 91 99 108 85 35 42 97 126 149 181 183 148 129 129 112 94 97 115 109 96 81 69 66 68 66 68 72 70 69 61 40 26 27 31 35 42 44 49 49 58 67 74 80 81 85 78 80 79 75 74 77 74 71 73 72 72 70 75 73 79 85 89 86 83 82 85 90 85 85 81 83 82 83 77 73 72 72 76 73 73 74 78 73 77 79 86 89 91 97 97 91 83 85 87 92 92 93 100 105 91 53 58 96 114 136 169 192 199 219 197 148 137 146 153 95 98 112 100 70 59 78 96 99 96 92 77 51 29 20 25 38 47 54 59 63 66 71 75 80 81 85 83 80 80 78 78 74 73 73 77 76 74 73 73 75 75 81 86 91 91 89 88 91 90 89 88 89 89 86 82 75 73 66 65 70 75 79 80 82 81 81 83 89 94 95 86 83 84 85 89 93 95 96 97 100 81 60 94 114 108 115 133 148 150 175 211 209 171 148 145 88 99 128 115 94 99 112 116 119 117 117 114 72 24 19 29 42 52 66 69 70 69 72 77 80 84 84 89 84 82 78 76 72 72 76 75 76 76 77 73 81 82 83 85 90 92 90 89 87 91 90 92 89 91 88 85 78 73 71 68 74 71 78 79 80 82 85 91 96 97 85 81 81 85 87 93 92 96 97 97 90 44 44 123 145 122 106 110 148 163 150 182 224 221 180 149 91 98 121 128 134 138 132 132 136 135 136 148 121 43 12 26 39 54 65 71 71 72 73 76 81 84 85 86 90 87 80 71 73 73 71 71 73 78 77 78 79 84 85 89 87 90 91 94 92 89 89 86 90 88 85 83 83 81 81 76 73 77 77 80 80 83 88 95 92 85 80 80 82 88 94 92 98 99 98 89 59 23 68 135 149 144 128 120 150 174 169 176 216 247 221 178 100 110 126 137 143 142 137 140 149 155 155 172 175 92 20 24 37 53 65 73 74 74 76 76 81 84 89 87 90 91 87 78 71 69 71 73 68 75 75 79 81 85 87 87 88 90 94 95 99 90 88 89 90 85 85 86 86 86 83 83 77 80 77 77 80 88 91 88 81 79 83 83 87 90 95 98 102 104 98 77 37 41 120 156 154 157 156 152 162 169 168 159 191 228 211 168\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (str, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mint\u001b[39m(img_id))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28mint\u001b[39m(img_id)])\n\u001b[0;32m----> 6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mImage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m21\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m21\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (str, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, (img_id, img_idx ) in enumerate(val_dataloader):\n",
    "    print(i, img_id, img_idx)\n",
    "    print(int(img_id))\n",
    "    print(df['Image'][int(img_id)])\n",
    "    y_pred = model(df['Image'][int(img_id)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (list, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!list of [int]!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!list of [int]!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m21\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m21\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (list, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!list of [int]!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!list of [int]!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n"
     ]
    }
   ],
   "source": [
    "model.forward([1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
